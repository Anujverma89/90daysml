{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4665cb0-87cc-409c-bd56-94cd343aa56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Metrics (Overfitted Model):\n",
      "Accuracy: 0.6400\n",
      "Precision: 0.6579\n",
      "Recall: 0.6864\n",
      "F1-Score: 0.6417\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Generate synthetic dataset\n",
    "def generate_data(n_samples=200):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(n_samples, 2) * 10\n",
    "    y = (X[:, 0] + X[:, 1] > 10).astype(int)\n",
    "    \n",
    "    # Add noise (to simulate real-world messiness)\n",
    "    noise = np.random.choice(n_samples, size=int(n_samples * 0.1), replace=False)\n",
    "    y[noise] = 1 - y[noise]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Simple Decision Tree-Like Overfit Model (manually overfit)\n",
    "def train_overfit_model(X, y):\n",
    "    thresholds = np.linspace(0, 10, 100)\n",
    "    best_acc = 0\n",
    "    best_split = (0, 5)  # (feature index, threshold)\n",
    "    \n",
    "    for feat in range(X.shape[1]):\n",
    "        for thresh in thresholds:\n",
    "            pred = (X[:, feat] > thresh).astype(int)\n",
    "            acc = (pred == y).mean()\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_split = (feat, thresh)\n",
    "                \n",
    "    return best_split\n",
    "\n",
    "def predict_model(X, model):\n",
    "    feat, thresh = model\n",
    "    return (X[:, feat] > thresh).astype(int)\n",
    "\n",
    "# Evaluation Metrics\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    \n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1\n",
    "    }\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "def cross_validate(X, y, k=5):\n",
    "    fold_size = len(X) // k\n",
    "    metrics_list = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end = (i+1) * fold_size\n",
    "        X_val = X[start:end]\n",
    "        y_val = y[start:end]\n",
    "        X_train = np.concatenate((X[:start], X[end:]), axis=0)\n",
    "        y_train = np.concatenate((y[:start], y[end:]), axis=0)\n",
    "\n",
    "        model = train_overfit_model(X_train, y_train)\n",
    "        y_pred = predict_model(X_val, model)\n",
    "        metrics = evaluate_metrics(y_val, y_pred)\n",
    "        metrics_list.append(metrics)\n",
    "    \n",
    "    # Average across folds\n",
    "    avg_metrics = {\n",
    "        metric: np.mean([m[metric] for m in metrics_list])\n",
    "        for metric in metrics_list[0]\n",
    "    }\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "# Run\n",
    "X, y = generate_data()\n",
    "metrics = cross_validate(X, y, k=5)\n",
    "print(\"Cross-Validation Metrics (Overfitted Model):\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f03eea-31af-4da8-bd7b-b9757a08729c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
