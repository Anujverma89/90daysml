{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27250a7c-80e0-4b82-92ba-37872be56e38",
   "metadata": {},
   "source": [
    "# Maths Refresher \n",
    "### **Analytics Geometry Continued**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566314ac-c307-4fb5-b202-a208547b0461",
   "metadata": {},
   "source": [
    "### Symmetric and Positive Definite Matrix: \n",
    "* **Symmetric Matrix**\n",
    "* Matrix whose transpose is equal to itself i.e. Aᵀ = A\n",
    "* **Positive Defininte Matrix**\n",
    "* A matrix A is positive definite if it satisfies this condition:\n",
    "* For any non-zero vector x, the quadratic form xᵀAx > 0 is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a346e31-49d5-4fc8-8246-7e148fa070c9",
   "metadata": {},
   "source": [
    "# Inner product induces Euclidean norm (Norm is used to find length or magnitude)\n",
    "* i.e inner product can be used as norm\n",
    "* **length of vector**\n",
    "* **Edclidean Norm l2**\n",
    "* Length of a vector is given by inner product of a vector by itself\n",
    "*  x= [1,1]ᵀ ||X|| = √ (x1)^2 + (x2)^2\n",
    "*  The above is known as euclidean norm\n",
    "* **Manhattan Norm l1**\n",
    "* Not induced by inner product \n",
    "* ||X|| = |x1| + |x2|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874001d-a499-4dd4-8058-0e3ca44e6db2",
   "metadata": {},
   "source": [
    "# Distance between two vectors : \n",
    "* **Eclidean**\n",
    "* d(x,y)= √ (x1-y1)^2 + (x2-y2)^2\n",
    "* **Manhattan**\n",
    "* d(x,y) = |x1-y1| + |x2-y2|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d810f6b9-00e1-4c26-99f6-1953294ab490",
   "metadata": {},
   "source": [
    "# Angle between two vectors : \n",
    "* cos(θ) = <x,y> / ||x|| * ||y||\n",
    "* <x,y> = inner product || dot product\n",
    "* ||x|| = norm (length or magnitute)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c3187-7cb4-4375-922d-2d83ad1bcf1e",
   "metadata": {},
   "source": [
    "#  The Cauchy-Schwarz Inequality:\n",
    "* This is where the Cauchy-Schwarz inequality is useful — it shows that the cosine of the angle between two vectors is always between \n",
    "−1and 1\n",
    "* ∣⟨x,y⟩∣≤∥x∥∥y∥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d861a927-562a-4639-8fee-d3f00b430de5",
   "metadata": {},
   "source": [
    "# Taking about the length and distance \n",
    "* Length is from origin to vector edge.\n",
    "* Length is found by taking a eclidean norm by itself.\n",
    "* Distance is between two vectors\n",
    "* Distance is found by subtracting one vector from other d(x,y) = x-y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e738fe1e-ca9a-41a9-a89d-db45ae3763a2",
   "metadata": {},
   "source": [
    "### N dimensional vector space : \n",
    "* Basis vectors : \n",
    "* A vector space where we have N basis vectors and each vectors have N dimensions\n",
    "* **NOTE** Vector space can be considered as a entire dataset\n",
    "* Each Vector is a data point\n",
    "* There are few independent vector which can power all the vector\n",
    "* We capture those in dimentionality reduction \n",
    "\n",
    "### Orthogonal Basis : \n",
    "* All the basis vector which have 90 degree angle between them.\n",
    "\n",
    "\n",
    "### Orthogonal Compliment \n",
    "* Consider a D dimensional vector space V\n",
    "* M dimensional sub space U\n",
    "* Now Orthogoanl compliment of U is all vectors in v that are orthogonal to every in U\n",
    "* Dimension of orthogoanl compliment is D-M\n",
    "* Used in PCA, SVM , Least Square Regressoin (LSR)\n",
    "* the orthogonal complement can also be used to describe a plane U (two-dimensional subspace) in a three-dimensional vector space.\n",
    "* It helps to caputre the high dimensional features into low dimensional space.\n",
    "\n",
    "\n",
    "### Inner Product of Functions  \n",
    "* **Orthogonality of functions** : Two functions \\(f(x)\\) and \\(g(x)\\) are orthogonal over an interval \\([a,b]\\) if their inner product is zero.\n",
    "* As we take inner product of a vector by taking dot product of it, we can take a inner product of a function by integrating over range of functions\n",
    "\n",
    "\n",
    "### Orthogonal projections\n",
    "* A projection is a way of dropping or mapping a vector onto a subspace (like a line or a plane), such that it stays as close as possible to the original vector.\n",
    "* Used in Dimentionality reduction, pca, Autoencoders.\n",
    "* Projection onto one diemensional subspace Line\n",
    "* Projection onto general subspace\n",
    "* Projection into affine space\n",
    "\n",
    "\n",
    "### Rotation : \n",
    "* Rotation in 2D space\n",
    "* Rotation in 3D space\n",
    "* Rotation in nD space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b40d9-d635-4488-adfe-4e8ee7fd1ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
