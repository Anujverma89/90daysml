{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf8e5871-0804-4b95-b28e-d665c02261fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Mechanism : Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004e652e-6a6b-4519-bba6-2ce332f887f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Output:\n",
      "[22.299, 7.609, 12.649, 15.634]\n",
      "[22.256, 8.09, 12.248, 15.595]\n",
      "[21.44, 7.615, 12.711, 15.377]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "# maths \n",
    "def dot(v1, v2):\n",
    "    return sum([x * y for x, y in zip(v1, v2)])\n",
    "\n",
    "def matmul(A, B):\n",
    "    return [[dot(row, col) for col in zip(*B)] for row in A]\n",
    "\n",
    "def relu(x):\n",
    "    return [max(0, val) for val in x]\n",
    "\n",
    "def softmax(x):\n",
    "    max_val = max(x)\n",
    "    exps = [math.exp(i - max_val) for i in x]\n",
    "    sum_exps = sum(exps)\n",
    "    return [j / sum_exps for j in exps]\n",
    "\n",
    "def add_vectors(a, b):\n",
    "    return [x + y for x, y in zip(a, b)]\n",
    "\n",
    "def transpose(m):\n",
    "    return list(map(list, zip(*m)))\n",
    "\n",
    "# encoding\n",
    "def positional_encoding(length, d_model):\n",
    "    pe = []\n",
    "    for pos in range(length):\n",
    "        row = []\n",
    "        for i in range(d_model):\n",
    "            angle = pos / (10000 ** (2 * (i // 2) / d_model))\n",
    "            if i % 2 == 0:\n",
    "                row.append(math.sin(angle))\n",
    "            else:\n",
    "                row.append(math.cos(angle))\n",
    "        pe.append(row)\n",
    "    return pe\n",
    "\n",
    "\n",
    "def linear(input_vec, weight, bias):\n",
    "    output = []\n",
    "    for w_row, b in zip(weight, bias):\n",
    "        output.append(dot(input_vec, w_row) + b)\n",
    "    return output\n",
    "\n",
    "\n",
    "def scaled_dot_attention(Q, K, V):\n",
    "    d_k = len(Q[0])\n",
    "    scores = matmul(Q, transpose(K))\n",
    "    scaled_scores = [[x / math.sqrt(d_k) for x in row] for row in scores]\n",
    "    attn_weights = [softmax(row) for row in scaled_scores]\n",
    "    output = matmul(attn_weights, V)\n",
    "    return output\n",
    "\n",
    "\n",
    "def feed_forward(x, w1, b1, w2, b2):\n",
    "    x1 = [relu(linear(vec, w1, b1)) for vec in x]\n",
    "    x2 = [linear(vec, w2, b2) for vec in x1]\n",
    "    return x2\n",
    "\n",
    "\n",
    "def transformer_encoder(x, QW, KW, VW, OW, OB, FF1W, FF1B, FF2W, FF2B):\n",
    "    # Self Attention\n",
    "    Q = [linear(vec, QW, [0]*len(QW)) for vec in x]\n",
    "    K = [linear(vec, KW, [0]*len(KW)) for vec in x]\n",
    "    V = [linear(vec, VW, [0]*len(VW)) for vec in x]\n",
    "    attn_out = scaled_dot_attention(Q, K, V)\n",
    "    \n",
    "    # Add & Norm (skipped real normalization for simplicity)\n",
    "    x = [add_vectors(a, b) for a, b in zip(x, attn_out)]\n",
    "    \n",
    "    # Feedforward\n",
    "    ff_out = feed_forward(x, FF1W, FF1B, FF2W, FF2B)\n",
    "    \n",
    "    # Add & Norm\n",
    "    x = [add_vectors(a, b) for a, b in zip(x, ff_out)]\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seq_len = 3\n",
    "    d_model = 4\n",
    "\n",
    "    # Dummy input (sequence of 3 vectors of size 4)\n",
    "    x = [[random.random() for _ in range(d_model)] for _ in range(seq_len)]\n",
    "\n",
    "    # Random weights\n",
    "    QW = [[random.random() for _ in range(d_model)] for _ in range(d_model)]\n",
    "    KW = [[random.random() for _ in range(d_model)] for _ in range(d_model)]\n",
    "    VW = [[random.random() for _ in range(d_model)] for _ in range(d_model)]\n",
    "    OW = [[random.random() for _ in range(d_model)] for _ in range(d_model)]\n",
    "    OB = [0] * d_model\n",
    "\n",
    "    FF1W = [[random.random() for _ in range(d_model)] for _ in range(8)]\n",
    "    FF1B = [0] * 8\n",
    "    FF2W = [[random.random() for _ in range(8)] for _ in range(d_model)]\n",
    "    FF2B = [0] * d_model\n",
    "\n",
    "    output = transformer_encoder(x, QW, KW, VW, OW, OB, FF1W, FF1B, FF2W, FF2B)\n",
    "    print(\"Transformer Output:\")\n",
    "    for row in output:\n",
    "        print([round(val, 3) for val in row])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96edd3-a53e-4734-ae09-808473d2d7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
