{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6744eff-6008-41b5-95e5-4b57c6960765",
   "metadata": {},
   "source": [
    "# Data can be represented into low dimension and high dimension : \n",
    "* Low dimension : Lower dimension approximation of a feature (PCA) \n",
    "* High dimension : Non linear higher dimension combination of a feature "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9bfd30d-d961-40ef-a99b-f85efe6d60b1",
   "metadata": {},
   "source": [
    "# Predictor (Models)\n",
    "### Predictor as a function \n",
    "### Predictor as a Probabilistic model \n",
    "\n",
    "# Learning in machine learning : \n",
    "* Prediction or inference\n",
    "* Training or parameter estimation\n",
    "*  Hyperparameter tuning or model selection\n",
    "# Predective models inferences output \n",
    "\n",
    "# Ways of finding good predictors : \n",
    "* Finding best predictor based on some measure of quality. (can be applied on both function and probabilistic model)\n",
    "* Baysian inference (Can only be applied on probabilistic model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f672738-026f-4759-8f1c-4f8b6033e825",
   "metadata": {},
   "source": [
    "# Machine learning part Part 1 (Supervised machine learning algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc0ad9-f571-4acd-9fe3-4d63c3730cf1",
   "metadata": {},
   "source": [
    "# Overview\n",
    "### Algorithms we will learn :\n",
    "* Regression\n",
    "  * Simgple Linear\n",
    "  * Multiple linear\n",
    "  * Polynomail regression \n",
    "* Classification\n",
    "  * KNN\n",
    "  * K Medoid\n",
    "  * Logistic Regression,\n",
    "  * Decision Trees,\n",
    "  * Support Vector Machines (SVMs),\n",
    "  * Naive Bayes,\n",
    "  * and Random Forests\n",
    "\n",
    "\n",
    "### Algorithms to avoid overfitting : \n",
    "* Regularization (L1 = Ridge, L2 = Lasso)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dea08b-6bef-4ea6-ac21-85596f4ddbd0",
   "metadata": {},
   "source": [
    "# Linear regression from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2491046a-1b74-46ae-a483-e13b47cdd442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression : ( Building Simple regression on my own) \n",
    "# simple linear regression : y = mx + b \n",
    "# Fits straight visual line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78771822-e289-4050-8dd4-c913309e5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# 1. LINEAR REGRESSION (GRADIENT DESCENT)\n",
    "\n",
    "class LinearRegressionScratch:\n",
    "    def __init__(self, lr=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "\n",
    "\n",
    "# 2. RIDGE REGRESSION (L2 REGULARIZATION)\n",
    "\n",
    "class RidgeRegressionScratch(LinearRegressionScratch):\n",
    "    def __init__(self, lr=0.01, n_iters=1000, alpha=1.0):\n",
    "        super().__init__(lr, n_iters)\n",
    "        self.alpha = alpha  # L2 penalty term\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            dw = (1 / n_samples) * (np.dot(X.T, (y_pred - y)) + self.alpha * self.weights)\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "\n",
    "# 3. LASSO REGRESSION (L1 REGULARIZATION)\n",
    "\n",
    "class LassoRegressionScratch(LinearRegressionScratch):\n",
    "    def __init__(self, lr=0.01, n_iters=1000, alpha=1.0):\n",
    "        super().__init__(lr, n_iters)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            # Subgradient of L1 norm\n",
    "            dw += self.alpha * np.sign(self.weights)\n",
    "\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a606f3-e298-422c-8f7e-c30ff67d8c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
